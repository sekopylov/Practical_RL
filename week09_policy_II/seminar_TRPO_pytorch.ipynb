{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "\n",
    "    !pip install -q gymnasium\n",
    "\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a TRPO!\n",
    "\n",
    "In this notebook we will write the code of the one Trust Region Policy Optimization.\n",
    "As usually, it contains a few different parts which we are going to reproduce.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space Box([ -1.        -1.        -1.        -1.       -12.566371 -28.274334], [ 1.        1.        1.        1.       12.566371 28.274334], (6,), float32)\n",
      "Action Space Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"Acrobot-v1\", render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "observation_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation Space\", env.observation_space)\n",
    "print(\"Action Space\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ea7a0f43490>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAGiCAYAAABd6zmYAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAI15JREFUeJzt3X10VOWh7/HfTF4m5GUmBMiklKTSSsUUoTUoTL3r9F5JiTZ98Yjr9ri4NrW0XjG4RHpdR04Vrz09NyxctdUWsbe14Do9yll0FVupqNygsR5DwAg1Qk3V0iYtTCLQzCSBTF7muX9Q5jgyYF4m2U82389as1az957JM7tMvu49z+zxGGOMAACwkNfpAQAAcC5ECgBgLSIFALAWkQIAWItIAQCsRaQAANYiUgAAaxEpAIC1iBQAwFpECgBgLccitXHjRl100UXKycnRokWLtHfvXqeGAgCwlCOR+vd//3etWbNG9913n1577TUtWLBAVVVV6uzsdGI4AABLeZy4wOyiRYt0xRVX6Ic//KEkKR6Pq7S0VLfffrvuvvvuiR4OAMBSmRP9C/v7+9Xc3Ky1a9cmlnm9XlVWVqqxsTHlfWKxmGKxWOLneDyuEydOaNq0afJ4POM+ZgBAehlj1N3drZkzZ8rrPfdJvQmP1LFjxzQ0NKRgMJi0PBgM6s0330x5n7q6Ot1///0TMTwAwARqb2/XrFmzzrl+wiM1GmvXrtWaNWsSP0ciEZWVlam9vV1+v9/BkQEARiMajaq0tFQFBQXn3W7CIzV9+nRlZGSoo6MjaXlHR4dKSkpS3sfn88nn85213O/3EykAmMQ+6C2bCZ/dl52drYqKCtXX1yeWxeNx1dfXKxQKTfRwAAAWc+R035o1a1RTU6OFCxfqyiuv1Pe//3319vbq5ptvdmI4AABLORKpL3/5y3r33Xe1bt06hcNhffKTn9Szzz571mQKAMCFzZHPSY1VNBpVIBBQJBLhPSkAmISG+3eca/cBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsNaII/XSSy/pC1/4gmbOnCmPx6Onnnoqab0xRuvWrdOHPvQhTZkyRZWVlXrrrbeStjlx4oSWL18uv9+vwsJCrVixQj09PWN6IgAA9xlxpHp7e7VgwQJt3Lgx5foNGzbo4Ycf1qOPPqqmpibl5eWpqqpKfX19iW2WL1+ugwcPateuXdqxY4deeukl3XLLLaN/FgAAdzJjIMls37498XM8HjclJSXmgQceSCzr6uoyPp/PPPnkk8YYYw4dOmQkmX379iW22blzp/F4POYvf/nLsH5vJBIxkkwkEhnL8AEADhnu3/G0vid1+PBhhcNhVVZWJpYFAgEtWrRIjY2NkqTGxkYVFhZq4cKFiW0qKyvl9XrV1NSU8nFjsZii0WjSDQDgfmmNVDgcliQFg8Gk5cFgMLEuHA6ruLg4aX1mZqaKiooS27xfXV2dAoFA4lZaWprOYQMALDUpZvetXbtWkUgkcWtvb3d6SACACZDWSJWUlEiSOjo6kpZ3dHQk1pWUlKizszNp/eDgoE6cOJHY5v18Pp/8fn/SDQDgfmmN1OzZs1VSUqL6+vrEsmg0qqamJoVCIUlSKBRSV1eXmpubE9vs3r1b8XhcixYtSudwAACTXOZI79DT06O333478fPhw4d14MABFRUVqaysTKtXr9Z3vvMdzZkzR7Nnz9a9996rmTNn6rrrrpMkXXrppbrmmmv0jW98Q48++qgGBga0atUq/cM//INmzpyZticGAHCBkU4bfOGFF4yks241NTXGmNPT0O+9914TDAaNz+czS5YsMa2trUmPcfz4cXPjjTea/Px84/f7zc0332y6u7vTPnURAGCn4f4d9xhjjIONHJVoNKpAIKBIJML7UwAwCQ337/ikmN0HALgwESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLUynR7AWDz55JOaMmWK08MAAIzQqVOnhrXdpI6UMUbGGKeHAQAYoeH+7faYSfhXPhqNKhAIKBKJyO/3Oz0cAMAIDffvOO9JAQCsRaQAANYiUgAAaxEpAIC1iBQAwFpECgBgLSIFALAWkQIAWItIAQCsRaQAANYiUgAAaxEpAIC1iBQAwFpECgBgLSIFALAWkQIAWItIAQCsRaQAANYiUgAAaxEpAIC1iBQAwFpECgBgLSIFALAWkQIAWItIAQCsRaQAANYiUgAAaxEpAIC1iBQAwFpECgBgLSIFALAWkQIAWItIAQCsRaQAANYiUgAAaxEpAIC1iBQAwFpECgBgLSIFALAWkQIAWItIAQCsRaQAANYaUaTq6up0xRVXqKCgQMXFxbruuuvU2tqatE1fX59qa2s1bdo05efna9myZero6Ejapq2tTdXV1crNzVVxcbHuuusuDQ4Ojv3ZAABcZUSRamhoUG1trfbs2aNdu3ZpYGBAS5cuVW9vb2KbO++8U08//bS2bdumhoYGHTlyRNdff31i/dDQkKqrq9Xf369XXnlFjz/+uLZs2aJ169al71kBANzBjEFnZ6eRZBoaGowxxnR1dZmsrCyzbdu2xDa/+93vjCTT2NhojDHmmWeeMV6v14TD4cQ2mzZtMn6/38RisWH93kgkYiSZSCQyluEDABwy3L/jY3pPKhKJSJKKiookSc3NzRoYGFBlZWVim7lz56qsrEyNjY2SpMbGRl122WUKBoOJbaqqqhSNRnXw4MGUvycWiykajSbdAADuN+pIxeNxrV69WldddZXmzZsnSQqHw8rOzlZhYWHStsFgUOFwOLHNewN1Zv2ZdanU1dUpEAgkbqWlpaMdNgBgEhl1pGpra/XGG29o69at6RxPSmvXrlUkEknc2tvbx/13AgCclzmaO61atUo7duzQSy+9pFmzZiWWl5SUqL+/X11dXUlHUx0dHSopKUlss3fv3qTHOzP778w27+fz+eTz+UYzVADAJDaiIyljjFatWqXt27dr9+7dmj17dtL6iooKZWVlqb6+PrGstbVVbW1tCoVCkqRQKKSWlhZ1dnYmttm1a5f8fr/Ky8vH8lwAAC4zoiOp2tpaPfHEE/rlL3+pgoKCxHtIgUBAU6ZMUSAQ0IoVK7RmzRoVFRXJ7/fr9ttvVygU0uLFiyVJS5cuVXl5uW666SZt2LBB4XBY99xzj2prazlaAgAk8RhjzLA39nhSLt+8ebO++tWvSjr9Yd5vfvObevLJJxWLxVRVVaVHHnkk6VTen/70J61cuVIvvvii8vLyVFNTo/Xr1yszc3jNjEajCgQCikQi8vv9wx0+AMASw/07PqJI2YJIAcDkNty/41y7DwBgLSIFALAWkQIAWItIAQCsRaQAANYiUgAAaxEpAIC1iBQAwFpECgBgLSIFALAWkQIAWItIAQCsRaQAANYiUgAAaxEpAIC1iBQAwFpECgBgLSIFALAWkQIAWCvT6QEA+E/GmHOu83g8EzgSwA5ECrCAMYMaHDyuaPQ5dXXtUF/fQQ0N9Sgzc7ry8hZq6tT/rtzcy5WREZDHwwkQXDiIFOCwePyUurqeUkfHQzp5cq+k/zyaGhho06lTr+n48X9VIHCNiovXKD//Ko6qcMHgP8kABxkT17vv/ljt7Xfq5MkmvTdQydudUlfXdrW13aaenhfPe1oQcBMiBTjEmEEdP75FR46s0+Bgx7Du09fXora2O9TT8x8yJj7OIwScR6QAh/T2Nikc/j+KxyMjul9fX4uOHv3fGhrqGp+BARYhUoAD4vGYIpGdisXeGdX9u7vrdfLkfk77wfWIFOCAgYE/q6Njw5geo63ttjSNBrAXkQIcYIyRMQNjfIy+NI0GsBeRAiaYMUYxTtMBw0KkAAd8/u23x/wYcSOdijPDD+5GpAAH9KfhSCouqZdIweWIFOCAv2qqdqh6TI/xmKkhUnA9IgU4oFd5ekH/TV0KjOr+bSrVbn1GPUQKLkekAIfs0WL9XDdoYISX0HxX07VRt+m4pnIkBdcjUoBDYsrRv+om7VD1sEMVUYF+oq/rN/o7DRiveoeGxnmUgLO4CjrgoF7l6fu6U8c0Q5/TrzVTR5Xq+uYDytQfNFs/0//QTn1OkkeZxuitWExXT/SggQlEpAAH/F1+vrb+9a+SPOpVvrboq9qnhbpaL+hTek2z9GdNUZ+i8uuwZus/dJVe1n/RH/RR6W8ZOxmPa29vr/7njBmOPhdgPBEpwAEVubl/i9RpMeXoNVXooOYpVyeVpQF5FdeQMtSvbPUqT4PKcnDEgDOIFOAAf0ZGiqUexZSjmHImfDyArZg4ATigwMtLDxgOXimAA1IfSQF4PyIFOIBIAcNDpAAHFKQxUnzxIdyMSAEOyE/Te1IxYzRApOBiRAqYYB5Pqo/rjk5/PM53U8HViBQwicWMScvXfgC2IlLAJNZnjGJcZBYuRqSASSwWj3MkBVcjUsAkxuk+uB2RAiaxY4ODOjE46PQwgHFDpAAH5Hq9mpcz9mv0vR2L6Q/9/WkYEWAnIgU4IM/r1SemTHF6GID1iBTgAK/HozwuMgt8IF4lgAMyJCIFDAOvEsABHEkBw8OrBHBAhsejPK6EDnwgIgU4wKvTM/zSwRjDldDhWkQKcIBX0pQ0RepkPC4SBbciUoADPB6P0nUt9J54XFy9D25FpIBJrpcjKbgYkQImuZ6hIcV5TwouRaSASY7TfXAzIgVMcj1DQ5zug2uNKFKbNm3S/Pnz5ff75ff7FQqFtHPnzsT6vr4+1dbWatq0acrPz9eyZcvU0dGR9BhtbW2qrq5Wbm6uiouLddddd2mQqzgDo9be369BTvfBpUYUqVmzZmn9+vVqbm7Wq6++qquvvlpf+tKXdPDgQUnSnXfeqaefflrbtm1TQ0ODjhw5ouuvvz5x/6GhIVVXV6u/v1+vvPKKHn/8cW3ZskXr1q1L77MCJoEPZ2VpRmbmmB9nd0+PTvHtvHApjxnjpwCLior0wAMP6IYbbtCMGTP0xBNP6IYbbpAkvfnmm7r00kvV2NioxYsXa+fOnfr85z+vI0eOKBgMSpIeffRR/eM//qPeffddZWdnD+t3RqNRBQIBRSIR+f3+sQwfcEzLyZO66Y9/1G9PnRrzY4Xnz1cwKysNowImxnD/jo/6PamhoSFt3bpVvb29CoVCam5u1sDAgCorKxPbzJ07V2VlZWpsbJQkNTY26rLLLksESpKqqqoUjUYTR2OpxGIxRaPRpBsw2eV4vcr2pOvTUoA7jThSLS0tys/Pl8/n06233qrt27ervLxc4XBY2dnZKiwsTNo+GAwqHA5LksLhcFKgzqw/s+5c6urqFAgEErfS0tKRDhuwjs/jIVLABxhxpC655BIdOHBATU1NWrlypWpqanTo0KHxGFvC2rVrFYlEErf29vZx/X3AROBICvhgI37XNjs7WxdffLEkqaKiQvv27dNDDz2kL3/5y+rv71dXV1fS0VRHR4dKSkokSSUlJdq7d2/S452Z/Xdmm1R8Pp98Pt9IhwpYzef1Kpuv6wDOa8yvkHg8rlgspoqKCmVlZam+vj6xrrW1VW1tbQqFQpKkUCiklpYWdXZ2JrbZtWuX/H6/ysvLxzoUYFLxeTzKStORFHP74FYjOpJau3atrr32WpWVlam7u1tPPPGEXnzxRT333HMKBAJasWKF1qxZo6KiIvn9ft1+++0KhUJavHixJGnp0qUqLy/XTTfdpA0bNigcDuuee+5RbW0tR0q44KTzPameoSGJ2X1woRFFqrOzU1/5yld09OhRBQIBzZ8/X88995w++9nPSpK+973vyev1atmyZYrFYqqqqtIjjzySuH9GRoZ27NihlStXKhQKKS8vTzU1Nfr2t7+d3mcFTAKeNL4fFR0aSttjATYZ8+eknMDnpOAWy955R7/o6hrz47zw8Y/rvxYUjH1AwAQZ989JAbAHR1JwKyIFuECESMGliBTgAt1ECi5FpAAX+MmxY04PARgXRApw0CU5OSP/RH0KJ7kKOlyKSAEO+nReHledAM6DVwfgoIKMDF6EwHnw+gAclO/1ikvMAudGpAAHESng/IgU4KD8jAx5+boO4JyIFOCgdB5JDU2+K5wBH4hIAQ7Ky8hIS6TikvqYhg4XIlKAg9L1Ahw0Rj1ECi5EpAAXiEs6RaTgQkQKcIEhY9RLpOBCRApwgSGJSMGViBTgAkPG6CRXQocLESnABY4PDurZaNTpYQBpR6QAh83JyRnzY3C6D25FpAAHeSRVFRQ4PQzAWkQKcJg/I8PpIQDWIlKAw4gUcG5ECnBYAZECzolIAQ7jSAo4NyIFOMyfpq+Pj0uKcyV0uAyRAhyWn6Yjqf54XINECi5DpAAHeTweZaTpSw9PGaN+IgWXIVKAS8TicSIF1yFSgEv0GaMBIgWXIVKAS/RxJAUXynR6AABSy1ZMl6hVc/R7zdAxeWT0V03V7zVHrZqrk8pL2r6P96TgQkQKsIxXQ/qY3tYt+r+6VG/Kr6hy1CdJ6le2ovLrHX1UP9XX9LoWaOhvL+P9J0/qnVhMH/P5nBw+kFZECnCYz+NRcWamOgcH5VOfrtUzulU/0jQd1/vn/eUophy9qxl6V/PVoh/r69qu69WrfPXG4+rjSuhwGd6TAhxWlJGhxXl58iiupXpet+pHmp4iUO/lkZSnk/qqtmiZfq4s9U/UcIEJRaQAh2V4PMr1ejVHb6lWGzVdx4d930JF9TVt1qe0fxxHCDiHSAEOy/B4VOTt12p9X9NGEKgz8tWru7VeU3RyHEYHOItIAQ7LkLTE26SL9dZ5T/Gdz3Qd07Xamc5hAVZg4gTgsEyPR3O8R9WvrlE/Rq5O6WN6J32DAizBkRTgsAyPR3lpuhI64Da8MgCHZUjKSWOkDB/ohYsQKcBh6bwSOt8nBbchUoAF2lSq4yoa9f17lavf6+MaSuOYABsQKcACTVqst3WxRnMcZCQd03Q9p6pR3R+wGZECLBCTT9/VGnWqeMT37VaBvqNvqU9TON0H1yFSgCUO66PaqFp1asaw7/NXFeoxrVCL5ksSp/vgOnxOCrCEkVf/T5Xyaki3aZNm6N1zfrjXSOpRvh7TCv1KX9Sgsk4vN0ZGGvWHggHbECnAIv3y6df6vA7qE/q6HtMndFBT9VdN0SlJUp986tJUvaU52qKv6qA+obgyEvfnSApuQ6QAyxh5dVgf0/26TxfrbX1Mbye+tqNLAb2jj+ktfVynlHvWfXlPCm5DpABL9cunQ/qEDukTw74P3yYFt2HiBOAiRApuQ6QAF4n/beIE4BZECnARjqTgNkQKcBEmTsBtiBTgIhxJwW2IFOAiHEnBbYgU4CIcScFtiBTgInGJ2X1wFSIFuAin++A2RApwEU73wW2IFOAiQxxJwWWIFOAiHEnBbYgU4CJcFgluQ6QAF+FICm4zpkitX79eHo9Hq1evTizr6+tTbW2tpk2bpvz8fC1btkwdHR1J92tra1N1dbVyc3NVXFysu+66S4ODg2MZCgARKbjPqCO1b98+/ehHP9L8+fOTlt955516+umntW3bNjU0NOjIkSO6/vrrE+uHhoZUXV2t/v5+vfLKK3r88ce1ZcsWrVu3bvTPAoAkpqDDfUYVqZ6eHi1fvlw//vGPNXXq1MTySCSixx57TA8++KCuvvpqVVRUaPPmzXrllVe0Z88eSdLzzz+vQ4cO6Wc/+5k++clP6tprr9U///M/a+PGjerv70/PswIuUBxJwW1GFana2lpVV1ersrIyaXlzc7MGBgaSls+dO1dlZWVqbGyUJDU2Nuqyyy5TMBhMbFNVVaVoNKqDBw+m/H2xWEzRaDTpBuBscWNkOJqCi4z46+O3bt2q1157Tfv27TtrXTgcVnZ2tgoLC5OWB4NBhcPhxDbvDdSZ9WfWpVJXV6f7779/pEMFLjgcScFtRnQk1d7erjvuuEP/9m//ppycnPEa01nWrl2rSCSSuLW3t0/Y7wYmEyIFtxlRpJqbm9XZ2anLL79cmZmZyszMVENDgx5++GFlZmYqGAyqv79fXV1dSffr6OhQSUmJJKmkpOSs2X5nfj6zzfv5fD75/f6kG4CzMXECbjOiSC1ZskQtLS06cOBA4rZw4UItX7488b+zsrJUX1+fuE9ra6va2toUCoUkSaFQSC0tLers7Exss2vXLvn9fpWXl6fpaQEXJo6k4DYjek+qoKBA8+bNS1qWl5enadOmJZavWLFCa9asUVFRkfx+v26//XaFQiEtXrxYkrR06VKVl5frpptu0oYNGxQOh3XPPfeotrZWPp8vTU8LuDBx7T64zYgnTnyQ733ve/J6vVq2bJlisZiqqqr0yCOPJNZnZGRox44dWrlypUKhkPLy8lRTU6Nvf/vb6R4KcMHh+6TgNh4zCeerRqNRBQIBRSIR3p+CK3y3o0P/689/HvPj1M6Yoe/OmiWflyuewW7D/TvOv2TARTjdB7chUoCLMHECbkOkABdhCjrchkgBLsLECbgNkQJchNN9cBsiBbgIp/vgNkQKsECe1yufxzPmx+FICm5DpAALLJgyRaXZ2WN+nIOnTmmQoym4CJECLOD5222sXidScBkiBVjA6/GkJVKA2xApwALpOpIC3IZIARbghQikxmsDsABHUkBqRAqwgMfjkScNU9ABtyFSgAW84kgKSIVIARbgdB+QGpECLMAUdCA1IgVYgCMpIDUiBVjAIzFxAkiBSAEW4IUIpMZrA7AAp/uA1IgUYAEPEyeAlIgUYAE+JwWkRqQAC3C6D0iNSAEW4LJIQGpECrAAp/uA1IgUYAECBaRGpAAL8J4UkBqRAizAtfuA1IgUYAGOpIDUiBRgAa7dB6RGpAALcLoPSI1IARYgUEBqRAqwAO9JAakRKcACfJgXSI1IARbgKuhAakQKsIBXzO4DUiFSgAV4TwpIjUgBFuB0H5AakQIswAsRSI3XBmABTvcBqREpwAJMQQdSI1KABfhmXiA1IgVYgNN9QGpECrAAp/uA1IgUYAECBaRGpAALeD0eXoxACrwuAAt4JImJE8BZiBRgASZOAKllOj0AAKdV5ObKjPExMiRlckQGFyFSgAU8Ho/+5cMfdnoYgHU43QcAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLUm5Vd1GHP6W3ei0ajDIwEAjMaZv99n/p6fy6SM1PHjxyVJpaWlDo8EADAW3d3dCgQC51w/KSNVVFQkSWprazvvk7vQRaNRlZaWqr29XX6/3+nhWIv9NDzsp+FhPw2PMUbd3d2aOXPmebeblJHyek+/lRYIBPhHMAx+v5/9NAzsp+FhPw0P++mDDecgg4kTAABrESkAgLUmZaR8Pp/uu+8++Xw+p4diNfbT8LCfhof9NDzsp/TymA+a/wcAgEMm5ZEUAODCQKQAANYiUgAAaxEpAIC1JmWkNm7cqIsuukg5OTlatGiR9u7d6/SQJtRLL72kL3zhC5o5c6Y8Ho+eeuqppPXGGK1bt04f+tCHNGXKFFVWVuqtt95K2ubEiRNavny5/H6/CgsLtWLFCvX09EzgsxhfdXV1uuKKK1RQUKDi4mJdd911am1tTdqmr69PtbW1mjZtmvLz87Vs2TJ1dHQkbdPW1qbq6mrl5uaquLhYd911lwYHByfyqYyrTZs2af78+YkPnoZCIe3cuTOxnn2U2vr16+XxeLR69erEMvbVODGTzNatW012drb56U9/ag4ePGi+8Y1vmMLCQtPR0eH00CbMM888Y771rW+ZX/ziF0aS2b59e9L69evXm0AgYJ566inz29/+1nzxi180s2fPNqdOnUpsc80115gFCxaYPXv2mN/85jfm4osvNjfeeOMEP5PxU1VVZTZv3mzeeOMNc+DAAfO5z33OlJWVmZ6ensQ2t956qyktLTX19fXm1VdfNYsXLzaf/vSnE+sHBwfNvHnzTGVlpdm/f7955plnzPTp083atWudeErj4le/+pX59a9/bX7/+9+b1tZW80//9E8mKyvLvPHGG8YY9lEqe/fuNRdddJGZP3++ueOOOxLL2VfjY9JF6sorrzS1tbWJn4eGhszMmTNNXV2dg6NyzvsjFY/HTUlJiXnggQcSy7q6uozP5zNPPvmkMcaYQ4cOGUlm3759iW127txpPB6P+ctf/jJhY59InZ2dRpJpaGgwxpzeJ1lZWWbbtm2JbX73u98ZSaaxsdEYc/o/BrxerwmHw4ltNm3aZPx+v4nFYhP7BCbQ1KlTzU9+8hP2UQrd3d1mzpw5ZteuXeYzn/lMIlLsq/EzqU739ff3q7m5WZWVlYllXq9XlZWVamxsdHBk9jh8+LDC4XDSPgoEAlq0aFFiHzU2NqqwsFALFy5MbFNZWSmv16umpqYJH/NEiEQikv7z4sTNzc0aGBhI2k9z585VWVlZ0n667LLLFAwGE9tUVVUpGo3q4MGDEzj6iTE0NKStW7eqt7dXoVCIfZRCbW2tqqurk/aJxL+n8TSpLjB77NgxDQ0NJf2fLEnBYFBvvvmmQ6OySzgclqSU++jMunA4rOLi4qT1mZmZKioqSmzjJvF4XKtXr9ZVV12lefPmSTq9D7Kzs1VYWJi07fv3U6r9eGadW7S0tCgUCqmvr0/5+fnavn27ysvLdeDAAfbRe2zdulWvvfaa9u3bd9Y6/j2Nn0kVKWA0amtr9cYbb+jll192eihWuuSSS3TgwAFFIhH9/Oc/V01NjRoaGpwellXa29t1xx13aNeuXcrJyXF6OBeUSXW6b/r06crIyDhrxkxHR4dKSkocGpVdzuyH8+2jkpISdXZ2Jq0fHBzUiRMnXLcfV61apR07duiFF17QrFmzEstLSkrU39+vrq6upO3fv59S7ccz69wiOztbF198sSoqKlRXV6cFCxbooYceYh+9R3Nzszo7O3X55ZcrMzNTmZmZamho0MMPP6zMzEwFg0H21TiZVJHKzs5WRUWF6uvrE8vi8bjq6+sVCoUcHJk9Zs+erZKSkqR9FI1G1dTUlNhHoVBIXV1dam5uTmyze/duxeNxLVq0aMLHPB6MMVq1apW2b9+u3bt3a/bs2UnrKyoqlJWVlbSfWltb1dbWlrSfWlpakoK+a9cu+f1+lZeXT8wTcUA8HlcsFmMfvceSJUvU0tKiAwcOJG4LFy7U8uXLE/+bfTVOnJ65MVJbt241Pp/PbNmyxRw6dMjccsstprCwMGnGjNt1d3eb/fv3m/379xtJ5sEHHzT79+83f/rTn4wxp6egFxYWml/+8pfm9ddfN1/60pdSTkH/1Kc+ZZqamszLL79s5syZ46op6CtXrjSBQMC8+OKL5ujRo4nbyZMnE9vceuutpqyszOzevdu8+uqrJhQKmVAolFh/Zsrw0qVLzYEDB8yzzz5rZsyY4aopw3fffbdpaGgwhw8fNq+//rq5++67jcfjMc8//7wxhn10Pu+d3WcM+2q8TLpIGWPMD37wA1NWVmays7PNlVdeafbs2eP0kCbUCy+8YCSddaupqTHGnJ6Gfu+995pgMGh8Pp9ZsmSJaW1tTXqM48ePmxtvvNHk5+cbv99vbr75ZtPd3e3AsxkfqfaPJLN58+bENqdOnTK33XabmTp1qsnNzTV///d/b44ePZr0OH/84x/Ntddea6ZMmWKmT59uvvnNb5qBgYEJfjbj52tf+5r5yEc+YrKzs82MGTPMkiVLEoEyhn10Pu+PFPtqfPBVHQAAa02q96QAABcWIgUAsBaRAgBYi0gBAKxFpAAA1iJSAABrESkAgLWIFADAWkQKAGAtIgUAsBaRAgBYi0gBAKz1/wE77JPfBwInKwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Defining a network\n",
    "\n",
    "With all it's complexity, at it's core TRPO is yet another policy gradient method.\n",
    "\n",
    "This essentially means we're actually training a stochastic policy $\\pi_\\theta \\left( a \\middle| s \\right)$.\n",
    "\n",
    "And yes, it's gonna be a neural network. So let's start by defining one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRPOAgent(nn.Module):\n",
    "    def __init__(self, state_shape: Tuple[int], n_actions: int, hidden_size=128):\n",
    "        '''\n",
    "        Here you should define your model\n",
    "        You should have LOG-PROBABILITIES as output because you will need it to compute loss\n",
    "        We recommend that you start simple:\n",
    "        use 1-2 hidden layers with 100-500 units and relu for the first try\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        assert isinstance(state_shape, tuple)\n",
    "        assert len(state_shape) == 1\n",
    "        input_dim = state_shape[0]\n",
    "        \n",
    "        # Prepare your model here.\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, states: torch.Tensor):\n",
    "        \"\"\"\n",
    "        takes agent's observation, returns log-probabilities\n",
    "        :param state_t: a batch of states, shape = [batch_size, state_shape]\n",
    "        \"\"\"\n",
    "\n",
    "        # Use your network to compute log_probs for the given states.\n",
    "        log_probs = self.net(states)\n",
    "        \n",
    "        return log_probs\n",
    "\n",
    "    def get_log_probs(self, states: torch.Tensor):\n",
    "        '''\n",
    "        Log-probs for training\n",
    "        '''\n",
    "        return self.forward(states)\n",
    "\n",
    "    def get_probs(self, states: torch.Tensor):\n",
    "        '''\n",
    "        Probs for interaction\n",
    "        '''\n",
    "        return torch.exp(self.forward(states))\n",
    "\n",
    "    def act(self, obs: np.ndarray, sample: bool = True):\n",
    "        '''\n",
    "        Samples action from policy distribution (sample = True) or takes most likely action (sample = False)\n",
    "        :param: obs - single observation vector\n",
    "        :param sample: if True, samples from \\pi, otherwise takes most likely action\n",
    "        :returns: action (single integer) and probabilities for all actions\n",
    "        '''\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = self.get_probs(torch.tensor(obs[np.newaxis], dtype=torch.float32)).numpy()\n",
    "\n",
    "        if sample:\n",
    "            action = int(np.random.choice(n_actions, p=probs[0]))\n",
    "        else:\n",
    "            action = int(np.argmax(probs))\n",
    "\n",
    "        return action, probs[0]\n",
    "\n",
    "\n",
    "agent = TRPOAgent(observation_shape, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled: [(2, array([0.38694307, 0.25832173, 0.35473523], dtype=float32)), (0, array([0.39185062, 0.25776443, 0.35038498], dtype=float32)), (2, array([0.38991147, 0.25658977, 0.35349876], dtype=float32)), (2, array([0.39531222, 0.25390762, 0.35078016], dtype=float32)), (0, array([0.38690656, 0.25931057, 0.35378292], dtype=float32))]\n",
      "greedy: [(0, array([0.3847694 , 0.26032823, 0.3549023 ], dtype=float32)), (0, array([0.3890949 , 0.25804645, 0.3528586 ], dtype=float32)), (0, array([0.3858767 , 0.26096684, 0.35315642], dtype=float32)), (0, array([0.38618776, 0.2598895 , 0.3539227 ], dtype=float32)), (0, array([0.39152458, 0.25668675, 0.35178864], dtype=float32))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sergei/study/rl/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Check if log-probabilities satisfies all the requirements\n",
    "log_probs = agent.get_log_probs(torch.tensor(env.reset()[0][np.newaxis], dtype=torch.float32))\n",
    "assert (\n",
    "    isinstance(log_probs, torch.Tensor) and\n",
    "    log_probs.requires_grad\n",
    "), \"log_probs must be a torch.Tensor with grad\"\n",
    "assert log_probs.shape == (1, n_actions)\n",
    "sums = torch.exp(log_probs).sum(dim=1)\n",
    "assert torch.allclose(sums, torch.ones_like(sums))\n",
    "\n",
    "# Demo use\n",
    "print(\"sampled:\", [agent.act(env.reset()[0]) for _ in range(5)])\n",
    "print(\"greedy:\", [agent.act(env.reset()[0], sample=False) for _ in range(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flat parameters operations\n",
    "\n",
    "We are going to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flat_params_from(model):\n",
    "    \"\"\"\n",
    "        get model params as tensor\n",
    "    \"\"\"\n",
    "    params = [torch.ravel(param.detach()) for param in model.parameters()]\n",
    "    flat_params = torch.cat(params)\n",
    "    return flat_params\n",
    "\n",
    "\n",
    "def set_flat_params_to(model, flat_params):\n",
    "    \"\"\"\n",
    "        set model params to flat_params\n",
    "    \"\"\"\n",
    "    prev_ind = 0\n",
    "    for param in model.parameters():\n",
    "        flat_size = int(np.prod(list(param.shape)))\n",
    "        param.data.copy_(\n",
    "            flat_params[prev_ind:prev_ind + flat_size].reshape(param.shape)\n",
    "        )\n",
    "        prev_ind += flat_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute cumulative reward just like you did in vanilla REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "\n",
    "\n",
    "def get_cumulative_returns(r, gamma=1):\n",
    "    \"\"\"\n",
    "    Computes cumulative discounted rewards given immediate rewards\n",
    "    G_i = r_i + gamma*r_{i+1} + gamma^2*r_{i+2} + ...\n",
    "    Also known as R(s,a).\n",
    "    \"\"\"\n",
    "    r = np.array(r)\n",
    "    assert r.ndim >= 1\n",
    "    return scipy.signal.lfilter([1], [1, -gamma], r[::-1], axis=0)[::-1]\n",
    "\n",
    "# we compute cumulative returns from end to beginning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.40049, 1.5561 , 1.729  , 0.81   , 0.9    , 1.     ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple demo on rewards [0,0,1,0,0,1]\n",
    "get_cumulative_returns([0, 0, 1, 0, 0, 1], gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rollout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, agent, max_pathlength=2500, n_timesteps=50000):\n",
    "    \"\"\"\n",
    "    Generate rollouts for training.\n",
    "    :param: env - environment in which we will make actions to generate rollouts.\n",
    "    :param: act - the function that can return policy and action given observation.\n",
    "    :param: max_pathlength - maximum size of one path that we generate.\n",
    "    :param: n_timesteps - total sum of sizes of all pathes we generate.\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "\n",
    "    total_timesteps = 0\n",
    "    while total_timesteps < n_timesteps:\n",
    "        obervations, actions, rewards, action_probs = [], [], [], []\n",
    "        obervation, _ = env.reset()\n",
    "        for _ in range(max_pathlength):\n",
    "            action, policy = agent.act(obervation)\n",
    "            obervations.append(obervation)\n",
    "            actions.append(action)\n",
    "            action_probs.append(policy)\n",
    "            obervation, reward, terminated, truncated, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            total_timesteps += 1\n",
    "            if terminated or truncated or total_timesteps >= n_timesteps:\n",
    "                path = {\n",
    "                    \"observations\": np.array(obervations),\n",
    "                    \"policy\": np.array(action_probs),\n",
    "                    \"actions\": np.array(actions),\n",
    "                    \"rewards\": np.array(rewards),\n",
    "                    \"cumulative_returns\": get_cumulative_returns(rewards),\n",
    "                }\n",
    "                paths.append(path)\n",
    "                break\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actions': array([0, 1, 1, 2, 0]),\n",
      " 'cumulative_returns': array([-5., -4., -3., -2., -1.]),\n",
      " 'observations': array([[ 0.9978647 , -0.06531441,  0.9967099 , -0.08105174, -0.09870496,\n",
      "         0.07371831],\n",
      "       [ 0.99792063, -0.06445447,  0.9950565 , -0.09931058,  0.10628925,\n",
      "        -0.25326484],\n",
      "       [ 0.9992618 , -0.03841639,  0.98973286, -0.14292943,  0.14784211,\n",
      "        -0.17472367],\n",
      "       [ 0.99996614, -0.00822874,  0.9864068 , -0.16432175,  0.14737935,\n",
      "        -0.03351954],\n",
      "       [ 0.9999886 ,  0.00477104,  0.99277174, -0.12001775, -0.0186686 ,\n",
      "         0.4754215 ]], dtype=float32),\n",
      " 'policy': array([[0.39741746, 0.25197405, 0.35060853],\n",
      "       [0.3867371 , 0.26033273, 0.35293016],\n",
      "       [0.38720232, 0.26072034, 0.35207736],\n",
      "       [0.3893696 , 0.25927526, 0.35135508],\n",
      "       [0.40376756, 0.2499138 , 0.34631875]], dtype=float32),\n",
      " 'rewards': array([-1., -1., -1., -1., -1.])}\n",
      "It's ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sergei/study/rl/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "paths = rollout(env, agent, max_pathlength=5, n_timesteps=100)\n",
    "pprint(paths[-1])\n",
    "\n",
    "assert (paths[0]['policy'].shape == (5, n_actions))\n",
    "assert (paths[0]['cumulative_returns'].shape == (5,))\n",
    "assert (paths[0]['rewards'].shape == (5,))\n",
    "assert (paths[0]['observations'].shape == (5,) + observation_shape)\n",
    "assert (paths[0]['actions'].shape == (5,))\n",
    "\n",
    "print(\"It's ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Auxiliary functions\n",
    "\n",
    "Now let's define the loss functions and something else for actual TRPO training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The surrogate reward should be:\n",
    "$$J_{surr}= {1 \\over N} \\sum\\limits_{i=1}^N \\frac{\\pi_{\\theta}(s_i, a_i)}{\\pi_{\\theta_{old}}(s_i, a_i)}A_{\\theta_{old}(s_i, a_i)}$$\n",
    "\n",
    "For simplicity, in this assignment we are going to use cumulative rewards instead of advantage:\n",
    "$$J'_{surr}= {1 \\over N} \\sum\\limits_{i=1}^N \\frac{\\pi_{\\theta}(s_i, a_i)}{\\pi_{\\theta_{old}}(s_i, a_i)}G_{\\theta_{old}(s_i, a_i)}$$\n",
    "\n",
    "Since we want to maximize the reward, we are going to minimize the corresponding surrogate loss:\n",
    "$$ L_{surr} = - J'_{surr} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(agent : TRPOAgent, observations, actions, cumulative_returns, old_probs):\n",
    "    \"\"\"\n",
    "    Computes TRPO objective\n",
    "    :param: observations - batch of observations [timesteps x state_shape]\n",
    "    :param: actions - batch of actions [timesteps]\n",
    "    :param: cumulative_returns - batch of cumulative returns [timesteps]\n",
    "    :param: old_probs - batch of probabilities computed by old network [timesteps x num_actions]\n",
    "    :returns: scalar value of the objective function\n",
    "\n",
    "    Here timesteps = batch_size\n",
    "    \"\"\"\n",
    "    batch_size = observations.shape[0]\n",
    "    probs_all = agent.get_probs(observations)\n",
    "\n",
    "    probs_for_actions = probs_all[torch.arange(batch_size), actions]  # probs_for_actions: [timesteps]\n",
    "    old_probs_for_actions = old_probs[torch.arange(batch_size), actions]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    loss = -torch.mean(cumulative_returns * (probs_for_actions / old_probs_for_actions))\n",
    "\n",
    "    assert loss.ndim == 0\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ascend these gradients as long as our $\\pi_\\theta(a|s)$ satisfies the constraint\n",
    "$$\\mathbb{E}_{s,\\pi_{\\theta_{t}}} \\Big[ \\operatorname{KL} \\left( \\pi_{\\theta_{t}} (s) \\:\\|\\: \\pi_{\\theta_{t+1}} (s) \\right) \\Big] < \\alpha$$\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "$$\\operatorname{KL} \\left( p \\| q \\right) = \\mathbb{E}_p \\log \\left( \\frac p q \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kl(agent, observations, actions, cumulative_returns, old_probs):\n",
    "    \"\"\"\n",
    "    Computes KL-divergence between network policy and old policy\n",
    "    :param: observations - batch of observations [timesteps x state_shape]\n",
    "    :param: actions - batch of actions [timesteps]\n",
    "    :param: cumulative_returns - batch of cumulative returns [timesteps] (we don't need it actually)\n",
    "    :param: old_probs - batch of probabilities computed by old network [timesteps x num_actions]\n",
    "    :returns: scalar value of the KL-divergence\n",
    "    \"\"\"\n",
    "    batch_size = observations.shape[0]\n",
    "    log_probs_all = agent.get_log_probs(observations)\n",
    "    probs_all = torch.exp(log_probs_all)  # [timesteps x num_actions]\n",
    "\n",
    "    # Compute Kullback-Leibler divergence (see formula above).\n",
    "    # Note: you need to sum KL and entropy over all actions, not just the ones agent took.\n",
    "    # You will also need to compute max KL over all timesteps. ???\n",
    "    old_log_probs = torch.log(old_probs + 1e-10)  # [timesteps x num_actions]\n",
    "\n",
    "    # we shouldn't count log of quotient, we use entropy and cross-entropy  \n",
    "    kl_divergences = torch.sum(old_probs * old_log_probs - old_probs * log_probs_all, dim=-1)\n",
    "    kl = torch.mean(kl_divergences)\n",
    "    # with torch.max works worse\n",
    "    assert kl.ndim == 0\n",
    "    assert (kl > -0.0001).all() and (kl < 10000).all()\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(agent, observations):\n",
    "    \"\"\"\n",
    "    Computes entropy of the network policy\n",
    "    :param: observations - batch of observations\n",
    "    :returns: scalar value of the entropy\n",
    "    \"\"\"\n",
    "\n",
    "    observations = torch.tensor(observations, dtype=torch.float32)\n",
    "\n",
    "    log_probs_all = agent.get_log_probs(observations)\n",
    "    probs_all = torch.exp(log_probs_all)\n",
    "\n",
    "    entropy = (-probs_all * log_probs_all).sum(dim=1).mean(dim=0)\n",
    "\n",
    "    assert entropy.ndim == 0\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear search**\n",
    "\n",
    "TRPO in its core involves ascending surrogate policy gradient constrained by KL divergence.\n",
    "\n",
    "In order to enforce this constraint, we're gonna use linesearch. You can find out more about it [here](https://en.wikipedia.org/wiki/Linear_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linesearch(f, x: torch.Tensor, fullstep: torch.Tensor, max_kl: float, max_backtracks: int = 10, backtrack_coef: float = 0.5):\n",
    "    \"\"\"\n",
    "    Linesearch finds the best parameters of neural networks in the direction of fullstep contrainted by KL divergence.\n",
    "    :param: f - function that returns loss, kl and arbitrary third component.\n",
    "    :param: x - old parameters of neural network.\n",
    "    :param: fullstep - direction in which we make search.\n",
    "    :param: max_kl - constraint of KL divergence.\n",
    "    :returns:\n",
    "    \"\"\"\n",
    "    loss, _, = f(x)\n",
    "    # By means of backtrack_coef**np.arange(max_backtracks) we\n",
    "    # do step /= 2 each time (if backtrack_coef = 0.5)\n",
    "    # here we make assumption that our loss function become bigger in the beginning\n",
    "    # and after that decrease\n",
    "    for stepfrac in backtrack_coef**np.arange(max_backtracks):\n",
    "        xnew = x + stepfrac * fullstep\n",
    "        new_loss, kl = f(xnew)\n",
    "        if kl <= max_kl and new_loss < loss:\n",
    "            x = xnew\n",
    "            loss = new_loss\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conjugate gradients**\n",
    "\n",
    "Since TRPO includes contrainted optimization, we will need to solve $A x = b$ using conjugate gradients.\n",
    "\n",
    "In general, CG is an algorithm that solves $A x = b$ where $A$ is positive-defined. $A$ is the Hessian matrix so $A$ is positive-defined. You can find out more about CG [here](https://en.wikipedia.org/wiki/Conjugate_gradient_method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(f_Ax, b, cg_iters=10, residual_tol=1e-10):\n",
    "    \"\"\"\n",
    "    This method solves system of equation Ax=b using an iterative method called conjugate gradients\n",
    "    :f_Ax: function that returns Ax\n",
    "    :b: targets for Ax\n",
    "    :cg_iters: how many iterations this method should do\n",
    "    :residual_tol: epsilon for stability\n",
    "    \"\"\"\n",
    "    p = b.clone()\n",
    "    r = b.clone()\n",
    "    x = torch.zeros_like(b)\n",
    "    rdotr = torch.sum(r*r)\n",
    "    for i in range(cg_iters):\n",
    "        z = f_Ax(p)\n",
    "        v = rdotr / (torch.sum(p*z) + 1e-8)\n",
    "        x += v * p\n",
    "        r -= v * z\n",
    "        newrdotr = torch.sum(r*r)\n",
    "        mu = newrdotr / (rdotr + 1e-8)\n",
    "        p = r + mu * p\n",
    "        rdotr = newrdotr\n",
    "        if rdotr < residual_tol:\n",
    "            break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 12.31049299  -5.80040259   7.21885867  27.11549842  13.83558373\n",
      " -44.58295352   3.78992048   6.11815418]\n",
      "[ 12.312775   -5.798267    7.2232366  27.109987   13.829966  -44.578644\n",
      "   3.7849069   6.116718 ]\n"
     ]
    }
   ],
   "source": [
    "# This code validates conjugate gradients\n",
    "A = np.random.rand(8, 8)\n",
    "A = A.T @ A\n",
    "\n",
    "\n",
    "def f_Ax(x):\n",
    "    return torch.ravel(torch.tensor(A, dtype=torch.float32) @ x.reshape(-1, 1))\n",
    "\n",
    "\n",
    "b = np.random.rand(8)\n",
    "w = (np.linalg.inv(A.T @ A) @ A.T @ b.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "print(w)\n",
    "print(conjugate_gradient(f_Ax, torch.tensor(b, dtype=torch.float32)).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: training\n",
    "In this section we construct the whole update step function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_step(agent, observations, actions, cumulative_returns, old_probs, max_kl):\n",
    "    \"\"\"\n",
    "    This function does the TRPO update step\n",
    "    :param: observations - batch of observations\n",
    "    :param: actions - batch of actions\n",
    "    :param: cumulative_returns - batch of cumulative returns\n",
    "    :param: old_probs - batch of probabilities computed by old network\n",
    "    :param: max_kl - controls how big KL divergence may be between old and new policy every step.\n",
    "    :returns: KL between new and old policies and the value of the loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    # Here we prepare the information\n",
    "    observations = torch.tensor(observations, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int64)\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float32)\n",
    "\n",
    "    # Here we compute gradient of the loss function\n",
    "    loss = get_loss(agent, observations, actions, cumulative_returns, old_probs)\n",
    "    grads = torch.autograd.grad(loss, agent.parameters())\n",
    "    loss_grad = torch.cat([torch.ravel(grad.detach()) for grad in grads])\n",
    "\n",
    "    def Fvp(v):\n",
    "        # Here we compute Fx to do solve Fx = g using conjugate gradients\n",
    "        # We actually do here a couple of tricks to compute it efficiently\n",
    "\n",
    "        kl = get_kl(agent, observations, actions, cumulative_returns, old_probs)\n",
    "\n",
    "        grads = torch.autograd.grad(kl, agent.parameters(), create_graph=True)\n",
    "        flat_grad_kl = torch.cat([grad.reshape(-1) for grad in grads])\n",
    "\n",
    "        kl_v = (flat_grad_kl * v).sum()\n",
    "        grads = torch.autograd.grad(kl_v, agent.parameters())\n",
    "        flat_grad_grad_kl = torch.cat([torch.ravel(grad) for grad in grads]).detach()\n",
    "\n",
    "        return flat_grad_grad_kl + v * 0.1\n",
    "\n",
    "    # Here we solve Fx = g system using conjugate gradients\n",
    "    stepdir = conjugate_gradient(Fvp, -loss_grad, 10)\n",
    "\n",
    "    # Here we compute the initial vector to do linear search\n",
    "    shs = 0.5 * (stepdir * Fvp(stepdir)).sum(0, keepdim=True)\n",
    "\n",
    "    lm = torch.sqrt(shs / max_kl)\n",
    "    fullstep = stepdir / lm[0]\n",
    "\n",
    "    neggdotstepdir = (-loss_grad * stepdir).sum(0, keepdim=True)\n",
    "\n",
    "    # Here we get the start point\n",
    "    prev_params = get_flat_params_from(agent)\n",
    "\n",
    "    def get_loss_kl(params):\n",
    "        # Helper for linear search\n",
    "        set_flat_params_to(agent, params)\n",
    "        return [\n",
    "            get_loss(agent, observations, actions, cumulative_returns, old_probs),\n",
    "            get_kl(agent, observations, actions, cumulative_returns, old_probs),\n",
    "        ]\n",
    "\n",
    "    # Here we find our new parameters\n",
    "    new_params = linesearch(get_loss_kl, prev_params, fullstep, max_kl)\n",
    "\n",
    "    # And we set it to our network\n",
    "    set_flat_params_to(agent, new_params)\n",
    "\n",
    "    return get_loss_kl(new_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Main TRPO loop\n",
    "\n",
    "Here we will train our network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** Iteration 1 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 100\n",
      "Average sum of rewards per episode:       -500.0\n",
      "Std of rewards per episode:               0.0\n",
      "Time elapsed:                             0.15 mins\n",
      "KL between old and new distribution:      0.009991743\n",
      "Entropy:                                  1.0557886\n",
      "Surrogate loss:                           249.59377\n",
      "\n",
      "********** Iteration 2 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 201\n",
      "Average sum of rewards per episode:       -495.03960396039605\n",
      "Std of rewards per episode:               41.04330502008659\n",
      "Time elapsed:                             0.29 mins\n",
      "KL between old and new distribution:      0.00998801\n",
      "Entropy:                                  1.0334133\n",
      "Surrogate loss:                           249.06786\n",
      "\n",
      "********** Iteration 3 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 301\n",
      "Average sum of rewards per episode:       -500.0\n",
      "Std of rewards per episode:               0.0\n",
      "Time elapsed:                             0.43 mins\n",
      "KL between old and new distribution:      0.0099948915\n",
      "Entropy:                                  1.0171744\n",
      "Surrogate loss:                           249.82024\n",
      "\n",
      "********** Iteration 4 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 413\n",
      "Average sum of rewards per episode:       -445.95535714285717\n",
      "Std of rewards per episode:               74.52796555570565\n",
      "Time elapsed:                             0.57 mins\n",
      "KL between old and new distribution:      0.009984431\n",
      "Entropy:                                  0.96350706\n",
      "Surrogate loss:                           228.7846\n",
      "\n",
      "********** Iteration 5 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 534\n",
      "Average sum of rewards per episode:       -412.4793388429752\n",
      "Std of rewards per episode:               77.12875348687353\n",
      "Time elapsed:                             0.72 mins\n",
      "KL between old and new distribution:      0.009999376\n",
      "Entropy:                                  0.9131438\n",
      "Surrogate loss:                           212.91927\n",
      "\n",
      "********** Iteration 6 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 684\n",
      "Average sum of rewards per episode:       -332.41333333333336\n",
      "Std of rewards per episode:               90.2785457471609\n",
      "Time elapsed:                             0.86 mins\n",
      "KL between old and new distribution:      0.009992806\n",
      "Entropy:                                  0.82777435\n",
      "Surrogate loss:                           177.66512\n",
      "\n",
      "********** Iteration 7 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 854\n",
      "Average sum of rewards per episode:       -293.1823529411765\n",
      "Std of rewards per episode:               86.23870248460591\n",
      "Time elapsed:                             1.01 mins\n",
      "KL between old and new distribution:      0.009973808\n",
      "Entropy:                                  0.8079936\n",
      "Surrogate loss:                           158.5367\n",
      "\n",
      "********** Iteration 8 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1041\n",
      "Average sum of rewards per episode:       -266.4117647058824\n",
      "Std of rewards per episode:               78.87122429644099\n",
      "Time elapsed:                             1.16 mins\n",
      "KL between old and new distribution:      0.0099871\n",
      "Entropy:                                  0.7666274\n",
      "Surrogate loss:                           144.31694\n",
      "\n",
      "********** Iteration 9 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1263\n",
      "Average sum of rewards per episode:       -224.25675675675674\n",
      "Std of rewards per episode:               73.31262089350078\n",
      "Time elapsed:                             1.30 mins\n",
      "KL between old and new distribution:      0.009997226\n",
      "Entropy:                                  0.73626626\n",
      "Surrogate loss:                           123.49092\n",
      "\n",
      "********** Iteration 10 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1512\n",
      "Average sum of rewards per episode:       -199.8273092369478\n",
      "Std of rewards per episode:               67.6400102092926\n",
      "Time elapsed:                             1.44 mins\n",
      "KL between old and new distribution:      0.009997816\n",
      "Entropy:                                  0.72118515\n",
      "Surrogate loss:                           110.91028\n",
      "\n",
      "********** Iteration 11 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1784\n",
      "Average sum of rewards per episode:       -182.85294117647058\n",
      "Std of rewards per episode:               75.05674612190636\n",
      "Time elapsed:                             1.59 mins\n",
      "KL between old and new distribution:      0.0099930465\n",
      "Entropy:                                  0.6432628\n",
      "Surrogate loss:                           106.34378\n",
      "\n",
      "********** Iteration 12 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 2113\n",
      "Average sum of rewards per episode:       -150.99088145896656\n",
      "Std of rewards per episode:               53.57076315092566\n",
      "Time elapsed:                             1.73 mins\n",
      "KL between old and new distribution:      0.009998365\n",
      "Entropy:                                  0.643245\n",
      "Surrogate loss:                           84.62288\n",
      "\n",
      "********** Iteration 13 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 2479\n",
      "Average sum of rewards per episode:       -135.62295081967213\n",
      "Std of rewards per episode:               51.12200041872177\n",
      "Time elapsed:                             1.87 mins\n",
      "KL between old and new distribution:      0.009998889\n",
      "Entropy:                                  0.6119326\n",
      "Surrogate loss:                           77.050644\n",
      "\n",
      "********** Iteration 14 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 2907\n",
      "Average sum of rewards per episode:       -115.82476635514018\n",
      "Std of rewards per episode:               27.235467593541845\n",
      "Time elapsed:                             2.02 mins\n",
      "KL between old and new distribution:      0.009992517\n",
      "Entropy:                                  0.6026696\n",
      "Surrogate loss:                           60.869164\n",
      "\n",
      "********** Iteration 15 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 3334\n",
      "Average sum of rewards per episode:       -116.10070257611241\n",
      "Std of rewards per episode:               36.59496227820694\n",
      "Time elapsed:                             2.16 mins\n",
      "KL between old and new distribution:      0.009997942\n",
      "Entropy:                                  0.56094444\n",
      "Surrogate loss:                           63.51194\n",
      "\n",
      "********** Iteration 16 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 3770\n",
      "Average sum of rewards per episode:       -113.68807339449542\n",
      "Std of rewards per episode:               42.10596010192268\n",
      "Time elapsed:                             2.30 mins\n",
      "KL between old and new distribution:      0.009984826\n",
      "Entropy:                                  0.5471081\n",
      "Surrogate loss:                           64.31004\n",
      "\n",
      "********** Iteration 17 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 4208\n",
      "Average sum of rewards per episode:       -113.15753424657534\n",
      "Std of rewards per episode:               36.65512884484774\n",
      "Time elapsed:                             2.44 mins\n",
      "KL between old and new distribution:      0.009986411\n",
      "Entropy:                                  0.52884823\n",
      "Surrogate loss:                           62.12576\n",
      "\n",
      "********** Iteration 18 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 4665\n",
      "Average sum of rewards per episode:       -108.41575492341357\n",
      "Std of rewards per episode:               39.55056610384592\n",
      "Time elapsed:                             2.59 mins\n",
      "KL between old and new distribution:      0.009995449\n",
      "Entropy:                                  0.48032936\n",
      "Surrogate loss:                           61.15514\n",
      "\n",
      "********** Iteration 19 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 5147\n",
      "Average sum of rewards per episode:       -102.73858921161826\n",
      "Std of rewards per episode:               34.34771738115916\n",
      "Time elapsed:                             2.73 mins\n",
      "KL between old and new distribution:      0.009999484\n",
      "Entropy:                                  0.43438086\n",
      "Surrogate loss:                           56.83195\n",
      "\n",
      "********** Iteration 20 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 5632\n",
      "Average sum of rewards per episode:       -102.09896907216495\n",
      "Std of rewards per episode:               42.37995172502605\n",
      "Time elapsed:                             2.87 mins\n",
      "KL between old and new distribution:      0.009969298\n",
      "Entropy:                                  0.42454898\n",
      "Surrogate loss:                           59.443104\n",
      "\n",
      "********** Iteration 21 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 6103\n",
      "Average sum of rewards per episode:       -105.1656050955414\n",
      "Std of rewards per episode:               44.67174645836257\n",
      "Time elapsed:                             3.01 mins\n",
      "KL between old and new distribution:      0.00998106\n",
      "Entropy:                                  0.39056525\n",
      "Surrogate loss:                           61.762085\n",
      "\n",
      "********** Iteration 22 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 6597\n",
      "Average sum of rewards per episode:       -100.22064777327935\n",
      "Std of rewards per episode:               40.00624823144628\n",
      "Time elapsed:                             3.16 mins\n",
      "KL between old and new distribution:      0.009977555\n",
      "Entropy:                                  0.38839734\n",
      "Surrogate loss:                           57.77801\n",
      "\n",
      "********** Iteration 23 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 7103\n",
      "Average sum of rewards per episode:       -97.8201581027668\n",
      "Std of rewards per episode:               39.50725741072513\n",
      "Time elapsed:                             3.30 mins\n",
      "KL between old and new distribution:      0.009980445\n",
      "Entropy:                                  0.39662647\n",
      "Surrogate loss:                           56.57711\n",
      "\n",
      "********** Iteration 24 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 7611\n",
      "Average sum of rewards per episode:       -97.43110236220473\n",
      "Std of rewards per episode:               42.613086890946406\n",
      "Time elapsed:                             3.45 mins\n",
      "KL between old and new distribution:      0.009994476\n",
      "Entropy:                                  0.34678155\n",
      "Surrogate loss:                           57.7697\n",
      "\n",
      "********** Iteration 25 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 8155\n",
      "Average sum of rewards per episode:       -90.91360294117646\n",
      "Std of rewards per episode:               24.346946123656384\n",
      "Time elapsed:                             3.59 mins\n",
      "KL between old and new distribution:      0.009989779\n",
      "Entropy:                                  0.37339142\n",
      "Surrogate loss:                           48.53535\n",
      "\n",
      "********** Iteration 26 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 8694\n",
      "Average sum of rewards per episode:       -91.76808905380334\n",
      "Std of rewards per episode:               32.26967798301534\n",
      "Time elapsed:                             3.73 mins\n",
      "KL between old and new distribution:      0.009979619\n",
      "Entropy:                                  0.33743218\n",
      "Surrogate loss:                           51.34415\n",
      "\n",
      "********** Iteration 27 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 9252\n",
      "Average sum of rewards per episode:       -88.60752688172043\n",
      "Std of rewards per episode:               23.645645987589855\n",
      "Time elapsed:                             3.88 mins\n",
      "KL between old and new distribution:      0.009996962\n",
      "Entropy:                                  0.29312703\n",
      "Surrogate loss:                           47.290257\n",
      "\n",
      "********** Iteration 28 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 9802\n",
      "Average sum of rewards per episode:       -89.91090909090909\n",
      "Std of rewards per episode:               28.5967777760634\n",
      "Time elapsed:                             4.02 mins\n",
      "KL between old and new distribution:      0.009990828\n",
      "Entropy:                                  0.24639134\n",
      "Surrogate loss:                           49.294617\n",
      "\n",
      "********** Iteration 29 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 10355\n",
      "Average sum of rewards per episode:       -89.42314647377938\n",
      "Std of rewards per episode:               39.32332208167469\n",
      "Time elapsed:                             4.16 mins\n",
      "KL between old and new distribution:      0.009983589\n",
      "Entropy:                                  0.25934243\n",
      "Surrogate loss:                           53.07528\n",
      "\n",
      "********** Iteration 30 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 10914\n",
      "Average sum of rewards per episode:       -88.44722719141323\n",
      "Std of rewards per episode:               30.215273927881825\n",
      "Time elapsed:                             4.30 mins\n",
      "KL between old and new distribution:      0.0099933995\n",
      "Entropy:                                  0.23320031\n",
      "Surrogate loss:                           49.136475\n",
      "\n",
      "********** Iteration 31 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 11469\n",
      "Average sum of rewards per episode:       -89.0954954954955\n",
      "Std of rewards per episode:               36.16339144231829\n",
      "Time elapsed:                             4.45 mins\n",
      "KL between old and new distribution:      0.009996683\n",
      "Entropy:                                  0.22907457\n",
      "Surrogate loss:                           51.654785\n",
      "\n",
      "********** Iteration 32 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 12019\n",
      "Average sum of rewards per episode:       -89.91272727272727\n",
      "Std of rewards per episode:               33.42567904822146\n",
      "Time elapsed:                             4.59 mins\n",
      "KL between old and new distribution:      0.009975515\n",
      "Entropy:                                  0.20943512\n",
      "Surrogate loss:                           50.919407\n",
      "\n",
      "********** Iteration 33 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 12577\n",
      "Average sum of rewards per episode:       -88.60931899641577\n",
      "Std of rewards per episode:               29.093435917999244\n",
      "Time elapsed:                             4.74 mins\n",
      "KL between old and new distribution:      0.009999114\n",
      "Entropy:                                  0.23370972\n",
      "Surrogate loss:                           48.86814\n",
      "\n",
      "********** Iteration 34 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 13131\n",
      "Average sum of rewards per episode:       -89.25451263537906\n",
      "Std of rewards per episode:               30.826318919388942\n",
      "Time elapsed:                             4.88 mins\n",
      "KL between old and new distribution:      0.009995752\n",
      "Entropy:                                  0.2342809\n",
      "Surrogate loss:                           49.71433\n",
      "\n",
      "********** Iteration 35 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 13684\n",
      "Average sum of rewards per episode:       -89.42314647377938\n",
      "Std of rewards per episode:               42.54343560061506\n",
      "Time elapsed:                             5.02 mins\n",
      "KL between old and new distribution:      0.009989407\n",
      "Entropy:                                  0.23550391\n",
      "Surrogate loss:                           54.581917\n",
      "\n",
      "********** Iteration 36 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 14244\n",
      "Average sum of rewards per episode:       -88.29107142857143\n",
      "Std of rewards per episode:               37.44738106800048\n",
      "Time elapsed:                             5.17 mins\n",
      "KL between old and new distribution:      0.009979231\n",
      "Entropy:                                  0.21687356\n",
      "Surrogate loss:                           51.83124\n",
      "\n",
      "********** Iteration 37 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 14801\n",
      "Average sum of rewards per episode:       -88.76840215439856\n",
      "Std of rewards per episode:               30.262618445613246\n",
      "Time elapsed:                             5.31 mins\n",
      "KL between old and new distribution:      0.0099937795\n",
      "Entropy:                                  0.21931222\n",
      "Surrogate loss:                           49.29345\n",
      "\n",
      "********** Iteration 38 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 15360\n",
      "Average sum of rewards per episode:       -88.4490161001789\n",
      "Std of rewards per episode:               33.03698520896565\n",
      "Time elapsed:                             5.46 mins\n",
      "KL between old and new distribution:      0.009989752\n",
      "Entropy:                                  0.20646898\n",
      "Surrogate loss:                           50.14091\n",
      "\n",
      "********** Iteration 39 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 15940\n",
      "Average sum of rewards per episode:       -85.20862068965518\n",
      "Std of rewards per episode:               23.511396975779114\n",
      "Time elapsed:                             5.60 mins\n",
      "KL between old and new distribution:      0.009977664\n",
      "Entropy:                                  0.18879034\n",
      "Surrogate loss:                           45.68461\n",
      "\n",
      "********** Iteration 40 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 16516\n",
      "Average sum of rewards per episode:       -85.80902777777777\n",
      "Std of rewards per episode:               27.19576001776146\n",
      "Time elapsed:                             5.75 mins\n",
      "KL between old and new distribution:      0.009974902\n",
      "Entropy:                                  0.18592258\n",
      "Surrogate loss:                           47.02643\n",
      "\n",
      "********** Iteration 41 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 17089\n",
      "Average sum of rewards per episode:       -86.26352530541013\n",
      "Std of rewards per episode:               28.120136428154396\n",
      "Time elapsed:                             5.89 mins\n",
      "KL between old and new distribution:      0.00999581\n",
      "Entropy:                                  0.16138712\n",
      "Surrogate loss:                           47.53448\n",
      "\n",
      "********** Iteration 42 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 17671\n",
      "Average sum of rewards per episode:       -84.91237113402062\n",
      "Std of rewards per episode:               21.550090731338607\n",
      "Time elapsed:                             6.04 mins\n",
      "KL between old and new distribution:      0.009982369\n",
      "Entropy:                                  0.16604976\n",
      "Surrogate loss:                           45.02603\n",
      "\n",
      "********** Iteration 43 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 18253\n",
      "Average sum of rewards per episode:       -84.91237113402062\n",
      "Std of rewards per episode:               20.709765402763914\n",
      "Time elapsed:                             6.18 mins\n",
      "KL between old and new distribution:      0.009992673\n",
      "Entropy:                                  0.17515148\n",
      "Surrogate loss:                           44.8102\n",
      "\n",
      "********** Iteration 44 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 18834\n",
      "Average sum of rewards per episode:       -85.06196213425129\n",
      "Std of rewards per episode:               28.090270314458667\n",
      "Time elapsed:                             6.32 mins\n",
      "KL between old and new distribution:      0.00999129\n",
      "Entropy:                                  0.16363263\n",
      "Surrogate loss:                           46.974224\n",
      "\n",
      "********** Iteration 45 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 19413\n",
      "Average sum of rewards per episode:       -85.35924006908463\n",
      "Std of rewards per episode:               28.007748683113135\n",
      "Time elapsed:                             6.47 mins\n",
      "KL between old and new distribution:      0.009979782\n",
      "Entropy:                                  0.18817887\n",
      "Surrogate loss:                           46.934525\n",
      "\n",
      "********** Iteration 46 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 20003\n",
      "Average sum of rewards per episode:       -83.74915254237288\n",
      "Std of rewards per episode:               24.743870138911316\n",
      "Time elapsed:                             6.61 mins\n",
      "KL between old and new distribution:      0.0021902693\n",
      "Entropy:                                  0.1837185\n",
      "Surrogate loss:                           45.44204\n",
      "\n",
      "********** Iteration 47 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 20589\n",
      "Average sum of rewards per episode:       -84.3259385665529\n",
      "Std of rewards per episode:               26.56972107576607\n",
      "Time elapsed:                             6.76 mins\n",
      "KL between old and new distribution:      0.004721167\n",
      "Entropy:                                  0.18557717\n",
      "Surrogate loss:                           46.2278\n",
      "\n",
      "********** Iteration 48 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 21173\n",
      "Average sum of rewards per episode:       -84.61986301369863\n",
      "Std of rewards per episode:               25.79275973554895\n",
      "Time elapsed:                             6.91 mins\n",
      "KL between old and new distribution:      0.009975507\n",
      "Entropy:                                  0.17321734\n",
      "Surrogate loss:                           46.02986\n",
      "\n",
      "********** Iteration 49 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 21751\n",
      "Average sum of rewards per episode:       -85.50865051903114\n",
      "Std of rewards per episode:               26.374480980065343\n",
      "Time elapsed:                             7.06 mins\n",
      "KL between old and new distribution:      0.009979006\n",
      "Entropy:                                  0.18967274\n",
      "Surrogate loss:                           46.605564\n",
      "\n",
      "********** Iteration 50 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 22342\n",
      "Average sum of rewards per episode:       -83.60406091370558\n",
      "Std of rewards per episode:               19.44071426125669\n",
      "Time elapsed:                             7.20 mins\n",
      "KL between old and new distribution:      0.009999492\n",
      "Entropy:                                  0.1895957\n",
      "Surrogate loss:                           43.87889\n",
      "\n",
      "********** Iteration 51 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 22909\n",
      "Average sum of rewards per episode:       -87.18871252204586\n",
      "Std of rewards per episode:               35.984780433641546\n",
      "Time elapsed:                             7.35 mins\n",
      "KL between old and new distribution:      0.009991569\n",
      "Entropy:                                  0.19614545\n",
      "Surrogate loss:                           50.7699\n",
      "\n",
      "********** Iteration 52 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 23501\n",
      "Average sum of rewards per episode:       -83.46114864864865\n",
      "Std of rewards per episode:               21.966847964408025\n",
      "Time elapsed:                             7.50 mins\n",
      "KL between old and new distribution:      0.009995889\n",
      "Entropy:                                  0.17082377\n",
      "Surrogate loss:                           44.40597\n",
      "\n",
      "********** Iteration 53 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 24090\n",
      "Average sum of rewards per episode:       -83.89134125636673\n",
      "Std of rewards per episode:               22.255998059942296\n",
      "Time elapsed:                             7.64 mins\n",
      "KL between old and new distribution:      0.009987229\n",
      "Entropy:                                  0.18199626\n",
      "Surrogate loss:                           44.702595\n",
      "\n",
      "********** Iteration 54 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 24675\n",
      "Average sum of rewards per episode:       -84.47179487179487\n",
      "Std of rewards per episode:               22.497399075326545\n",
      "Time elapsed:                             7.79 mins\n",
      "KL between old and new distribution:      0.009970899\n",
      "Entropy:                                  0.18275428\n",
      "Surrogate loss:                           45.05703\n",
      "\n",
      "********** Iteration 55 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 25252\n",
      "Average sum of rewards per episode:       -85.65857885615252\n",
      "Std of rewards per episode:               28.557756856293746\n",
      "Time elapsed:                             7.93 mins\n",
      "KL between old and new distribution:      0.004873498\n",
      "Entropy:                                  0.18156326\n",
      "Surrogate loss:                           47.464787\n",
      "\n",
      "********** Iteration 56 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 25847\n",
      "Average sum of rewards per episode:       -83.03697478991597\n",
      "Std of rewards per episode:               24.07422648319298\n",
      "Time elapsed:                             8.07 mins\n",
      "KL between old and new distribution:      0.009990408\n",
      "Entropy:                                  0.19820735\n",
      "Surrogate loss:                           44.84042\n",
      "\n",
      "********** Iteration 57 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 26443\n",
      "Average sum of rewards per episode:       -82.89597315436242\n",
      "Std of rewards per episode:               22.906124087179524\n",
      "Time elapsed:                             8.22 mins\n",
      "KL between old and new distribution:      0.009999649\n",
      "Entropy:                                  0.1839743\n",
      "Surrogate loss:                           44.45031\n",
      "\n",
      "********** Iteration 58 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 27034\n",
      "Average sum of rewards per episode:       -83.60406091370558\n",
      "Std of rewards per episode:               18.909319193860686\n",
      "Time elapsed:                             8.36 mins\n",
      "KL between old and new distribution:      0.0099873915\n",
      "Entropy:                                  0.16568625\n",
      "Surrogate loss:                           43.75707\n",
      "\n",
      "********** Iteration 59 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 27632\n",
      "Average sum of rewards per episode:       -82.61371237458194\n",
      "Std of rewards per episode:               18.21700148417577\n",
      "Time elapsed:                             8.51 mins\n",
      "KL between old and new distribution:      0.009993836\n",
      "Entropy:                                  0.16037703\n",
      "Surrogate loss:                           43.19665\n",
      "\n",
      "********** Iteration 60 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 28227\n",
      "Average sum of rewards per episode:       -83.03697478991597\n",
      "Std of rewards per episode:               28.146590657449437\n",
      "Time elapsed:                             8.65 mins\n",
      "KL between old and new distribution:      0.009989737\n",
      "Entropy:                                  0.16487272\n",
      "Surrogate loss:                           46.06897\n",
      "\n",
      "********** Iteration 61 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 28800\n",
      "Average sum of rewards per episode:       -86.26527050610821\n",
      "Std of rewards per episode:               34.99532950346333\n",
      "Time elapsed:                             8.79 mins\n",
      "KL between old and new distribution:      0.009979461\n",
      "Entropy:                                  0.1658754\n",
      "Surrogate loss:                           49.971836\n",
      "\n",
      "********** Iteration 62 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 29386\n",
      "Average sum of rewards per episode:       -84.3259385665529\n",
      "Std of rewards per episode:               25.695404887065294\n",
      "Time elapsed:                             8.94 mins\n",
      "KL between old and new distribution:      0.009982994\n",
      "Entropy:                                  0.16625126\n",
      "Surrogate loss:                           45.930386\n",
      "\n",
      "********** Iteration 63 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 29979\n",
      "Average sum of rewards per episode:       -83.31871838111299\n",
      "Std of rewards per episode:               22.135515564019293\n",
      "Time elapsed:                             9.08 mins\n",
      "KL between old and new distribution:      0.009992515\n",
      "Entropy:                                  0.1648884\n",
      "Surrogate loss:                           44.37056\n",
      "\n",
      "********** Iteration 64 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 30574\n",
      "Average sum of rewards per episode:       -83.03697478991597\n",
      "Std of rewards per episode:               27.328600666289443\n",
      "Time elapsed:                             9.23 mins\n",
      "KL between old and new distribution:      0.009996318\n",
      "Entropy:                                  0.16348001\n",
      "Surrogate loss:                           45.825626\n",
      "\n",
      "********** Iteration 65 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 31159\n",
      "Average sum of rewards per episode:       -84.47350427350428\n",
      "Std of rewards per episode:               28.273022970510375\n",
      "Time elapsed:                             9.38 mins\n",
      "KL between old and new distribution:      0.009997495\n",
      "Entropy:                                  0.15582243\n",
      "Surrogate loss:                           46.76103\n",
      "\n",
      "********** Iteration 66 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 31746\n",
      "Average sum of rewards per episode:       -84.18057921635435\n",
      "Std of rewards per episode:               26.97670326253626\n",
      "Time elapsed:                             9.53 mins\n",
      "KL between old and new distribution:      0.009984905\n",
      "Entropy:                                  0.16079356\n",
      "Surrogate loss:                           46.206467\n",
      "\n",
      "********** Iteration 67 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 32329\n",
      "Average sum of rewards per episode:       -84.76672384219555\n",
      "Std of rewards per episode:               30.643198536372104\n",
      "Time elapsed:                             9.67 mins\n",
      "KL between old and new distribution:      0.009985788\n",
      "Entropy:                                  0.18111235\n",
      "Surrogate loss:                           47.6543\n",
      "\n",
      "********** Iteration 68 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 32928\n",
      "Average sum of rewards per episode:       -82.47579298831386\n",
      "Std of rewards per episode:               24.955499592884028\n",
      "Time elapsed:                             9.82 mins\n",
      "KL between old and new distribution:      0.009996783\n",
      "Entropy:                                  0.16843805\n",
      "Surrogate loss:                           44.84147\n",
      "\n",
      "********** Iteration 69 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 33511\n",
      "Average sum of rewards per episode:       -84.76500857632934\n",
      "Std of rewards per episode:               27.822236691224152\n",
      "Time elapsed:                             9.97 mins\n",
      "KL between old and new distribution:      0.009989853\n",
      "Entropy:                                  0.16958816\n",
      "Surrogate loss:                           46.73849\n",
      "\n",
      "********** Iteration 70 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 34103\n",
      "Average sum of rewards per episode:       -83.46283783783784\n",
      "Std of rewards per episode:               29.15745878428858\n",
      "Time elapsed:                             10.11 mins\n",
      "KL between old and new distribution:      0.009995786\n",
      "Entropy:                                  0.17811964\n",
      "Surrogate loss:                           46.6428\n",
      "\n",
      "********** Iteration 71 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 34695\n",
      "Average sum of rewards per episode:       -83.46283783783784\n",
      "Std of rewards per episode:               30.309584561174717\n",
      "Time elapsed:                             10.25 mins\n",
      "KL between old and new distribution:      0.009976812\n",
      "Entropy:                                  0.19623117\n",
      "Surrogate loss:                           46.8617\n",
      "\n",
      "********** Iteration 72 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 35295\n",
      "Average sum of rewards per episode:       -82.335\n",
      "Std of rewards per episode:               23.76880536193044\n",
      "Time elapsed:                             10.40 mins\n",
      "KL between old and new distribution:      0.009981056\n",
      "Entropy:                                  0.19203761\n",
      "Surrogate loss:                           44.4076\n",
      "\n",
      "********** Iteration 73 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 35906\n",
      "Average sum of rewards per episode:       -80.83469721767594\n",
      "Std of rewards per episode:               18.09514336643107\n",
      "Time elapsed:                             10.54 mins\n",
      "KL between old and new distribution:      0.009971248\n",
      "Entropy:                                  0.20347084\n",
      "Surrogate loss:                           42.267826\n",
      "\n",
      "********** Iteration 74 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 36494\n",
      "Average sum of rewards per episode:       -84.0374149659864\n",
      "Std of rewards per episode:               28.994580096118863\n",
      "Time elapsed:                             10.69 mins\n",
      "KL between old and new distribution:      0.009999526\n",
      "Entropy:                                  0.19431289\n",
      "Surrogate loss:                           46.770645\n",
      "\n",
      "********** Iteration 75 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 37099\n",
      "Average sum of rewards per episode:       -81.64628099173554\n",
      "Std of rewards per episode:               18.535126541813618\n",
      "Time elapsed:                             10.84 mins\n",
      "KL between old and new distribution:      0.00999449\n",
      "Entropy:                                  0.15572323\n",
      "Surrogate loss:                           42.7969\n",
      "\n",
      "********** Iteration 76 ************\n",
      "Rollout\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Generating paths.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRollout\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m paths = \u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMade rollout\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Updating policy.\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mrollout\u001b[39m\u001b[34m(env, agent, max_pathlength, n_timesteps)\u001b[39m\n\u001b[32m     14\u001b[39m obervation, _ = env.reset()\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_pathlength):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     action, policy = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobervation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     obervations.append(obervation)\n\u001b[32m     18\u001b[39m     actions.append(action)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mTRPOAgent.act\u001b[39m\u001b[34m(self, obs, sample)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[33;03mSamples action from policy distribution (sample = True) or takes most likely action (sample = False)\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[33;03m:param: obs - single observation vector\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[33;03m:param sample: if True, samples from \\pi, otherwise takes most likely action\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m:returns: action (single integer) and probabilities for all actions\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     probs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.numpy()\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample:\n\u001b[32m     58\u001b[39m     action = \u001b[38;5;28mint\u001b[39m(np.random.choice(n_actions, p=probs[\u001b[32m0\u001b[39m]))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mTRPOAgent.get_probs\u001b[39m\u001b[34m(self, states)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_probs\u001b[39m(\u001b[38;5;28mself\u001b[39m, states: torch.Tensor):\n\u001b[32m     41\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m    Probs for interaction\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.exp(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mTRPOAgent.forward\u001b[39m\u001b[34m(self, states)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03mtakes agent's observation, returns log-probabilities\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03m:param state_t: a batch of states, shape = [batch_size, state_shape]\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Use your network to compute log_probs for the given states.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m log_probs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m log_probs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/rl/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/rl/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/rl/.conda/lib/python3.11/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/rl/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/rl/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/rl/.conda/lib/python3.11/site-packages/torch/nn/modules/activation.py:1748\u001b[39m, in \u001b[36mLogSoftmax.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1748\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/study/rl/.conda/lib/python3.11/site-packages/torch/nn/functional.py:2248\u001b[39m, in \u001b[36mlog_softmax\u001b[39m\u001b[34m(input, dim, _stacklevel, dtype)\u001b[39m\n\u001b[32m   2246\u001b[39m     dim = _get_softmax_dim(\u001b[33m\"\u001b[39m\u001b[33mlog_softmax\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m.dim(), _stacklevel)\n\u001b[32m   2247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2248\u001b[39m     ret = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2250\u001b[39m     ret = \u001b[38;5;28minput\u001b[39m.log_softmax(dim, dtype=dtype)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from itertools import count\n",
    "\n",
    "# TRPO hyperparameter; controls how big KL divergence may be between the old and the new policy at every step.\n",
    "max_kl = 0.01\n",
    "numeptotal = 0  # Number of episodes we have completed so far.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in count(1):\n",
    "    print(\"\\n********** Iteration %i ************\" % i)\n",
    "\n",
    "    # Generating paths.\n",
    "    print(\"Rollout\")\n",
    "    paths = rollout(env, agent)\n",
    "    print(\"Made rollout\")\n",
    "\n",
    "    # Updating policy.\n",
    "    observations = np.concatenate([path[\"observations\"] for path in paths])\n",
    "    actions = np.concatenate([path[\"actions\"] for path in paths])\n",
    "    returns = np.concatenate([path[\"cumulative_returns\"] for path in paths])\n",
    "    old_probs = np.concatenate([path[\"policy\"] for path in paths])\n",
    "\n",
    "    loss, kl = update_step(agent, observations, actions, returns, old_probs, max_kl)\n",
    "\n",
    "    # Report current progress\n",
    "    episode_rewards = np.array([path[\"rewards\"].sum() for path in paths])\n",
    "\n",
    "    stats = {}\n",
    "    numeptotal += len(episode_rewards)\n",
    "    stats[\"Total number of episodes\"] = numeptotal\n",
    "    stats[\"Average sum of rewards per episode\"] = episode_rewards.mean()\n",
    "    stats[\"Std of rewards per episode\"] = episode_rewards.std()\n",
    "    stats[\"Time elapsed\"] = \"%.2f mins\" % ((time.time() - start_time)/60.)\n",
    "    stats[\"KL between old and new distribution\"] = kl.data.numpy()\n",
    "    stats[\"Entropy\"] = get_entropy(agent, observations).data.numpy()\n",
    "    stats[\"Surrogate loss\"] = loss.data.numpy()\n",
    "    for k, v in stats.items():\n",
    "        print(k + \": \" + \" \" * (40 - len(k)) + str(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result:\\\n",
    "********** Iteration 75 ************\\\n",
    "Rollout\\\n",
    "Made rollout\\\n",
    "Total number of episodes:                 37099\\\n",
    "Average sum of rewards per episode:       **-81.64628099173554**\\\n",
    "Std of rewards per episode:               18.535126541813618\\\n",
    "Time elapsed:                             10.84 mins\\\n",
    "KL between old and new distribution:      0.00999449\\\n",
    "Entropy:                                  0.15572323\\\n",
    "Surrogate loss:                           42.7969\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework option I: better sampling (10+pts)\n",
    "\n",
    "In this section, you're invited to implement a better rollout strategy called _vine_.\n",
    "\n",
    "![img](https://s17.postimg.cc/i90chxgvj/vine.png)\n",
    "\n",
    "In most gym environments, you can actually backtrack by using states. You can find a wrapper that saves/loads states in [the MCTS seminar](https://github.com/yandexdataschool/Practical_RL/blob/master/week10_planning/seminar_MCTS.ipynb).\n",
    "\n",
    "You can read more about TRPO in the [original paper](https://arxiv.org/abs/1502.05477) in section 5.2.\n",
    "\n",
    "The goal here is to implement such rollout policy (we recommend using tree data structure like in the seminar above).\n",
    "Then you can assign cumulative rewards similar to `get_cumulative_rewards`, but for a tree.\n",
    "\n",
    "__bonus task__ - parallelize samples using multiple cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework option II (10+pts)\n",
    "\n",
    "Let's use TRPO to train evil robots! (pick any of two)\n",
    "* [MuJoCo robots](https://gymnasium.farama.org/environments/mujoco/#mujoco)\n",
    "* [Box2d robot](https://gymnasium.farama.org/environments/box2d/bipedal_walker/)\n",
    "\n",
    "The catch here is that those environments have continuous action spaces.\n",
    "\n",
    "Luckily, TRPO is a policy gradient method, so it's gonna work for any parametric $\\pi_\\theta(a|s)$. We recommend starting with gaussian policy:\n",
    "\n",
    "$$\\pi_\\theta(a|s) = N(\\mu_\\theta(s),\\sigma^2_\\theta(s)) = {1 \\over \\sqrt { 2 \\pi {\\sigma^2}_\\theta(s) } } e^{ (a -\n",
    "\\mu_\\theta(s))^2 \\over 2 {\\sigma^2}_\\theta(s) } $$\n",
    "\n",
    "In the $\\sqrt { 2 \\pi {\\sigma^2}_\\theta(s) }$ clause, $\\pi$ means ~3.1415926, not agent's policy.\n",
    "\n",
    "This essentially means that you will need two output layers:\n",
    "* $\\mu_\\theta(s)$, a dense layer with linear activation\n",
    "* ${\\sigma^2}_\\theta(s)$, a dense layer with activation tf.exp (to make it positive; like rho from bandits)\n",
    "\n",
    "For multidimensional actions, you can use a fully factorized gaussian (basically a vector of gaussians).\n",
    "\n",
    "__Bonus task__: compare the performance of the continuous action space method to action space discretization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
